{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmMFfkR0a1kNF/a3612Y1v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anumadhyani/mywork/blob/master/Detect_image_real_or_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7FMI_mMwjiV",
        "outputId": "71ecafad-67a5-4537-af44-204ff06a3caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNxsp8SFw-NB",
        "outputId": "4fee7025-d05a-43b2-c196-3b597bc08665"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import hashlib # Added for generating unique filenames\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.fftpack import fft2, fftshift\n",
        "from skimage.restoration import denoise_tv_chambolle\n",
        "\n",
        "from google.colab import userdata\n",
        "unsplashkey = userdata.get('Unsplash_Key')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Removed force_remount=True\n",
        "\n",
        "# Define the path where you want to save your images in Google Drive\n",
        "GDRIVE_SAVE_PATH = '/content/drive/MyDrive/Unsplash_Images'\n",
        "print(f\"Images will be saved to: {GDRIVE_SAVE_PATH}\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(GDRIVE_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "print(\"a\")\n",
        "\n",
        "def fetch_image_urls(query):\n",
        "\n",
        "  api_url =  f\"https://api.unsplash.com/search/photos/?query={query}&client_id={unsplashkey}\"\n",
        "  responses = requests.get(api_url)\n",
        "\n",
        "  if responses.status_code != 200:\n",
        "    print(\"Error fetching image data\")\n",
        "    return\n",
        "\n",
        "  data = responses.json()\n",
        "  print(\"hi\")\n",
        "  urls = [result[\"urls\"][\"regular\"] for result in data[\"results\"]]\n",
        "  print(\"here\")\n",
        "  return urls\n",
        "\n",
        "def download_and_save_images(image_url, base_save_path): # Renamed parameter for clarity\n",
        "  print(f\"Attempting to download: {image_url}\") # Clarified print statement\n",
        "  response = requests.get(image_url)\n",
        "  if response.status_code == 200:\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    #Determine file extension from content type or default to .jpg\n",
        "    content_type = response.headers.get('content-type', 'image/jpeg')\n",
        "    extension = '.' + content_type.split('/')[-1]\n",
        "\n",
        "    # Handle cases where content type might not provide a standard extension, or add more common types\n",
        "    if extension not in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:\n",
        "      extension = '.jpg' # Fallback extension\n",
        "\n",
        "    # Generate a unique filename using a hash of the URL\n",
        "    filename = hashlib.md5(image_url.encode('utf-8')).hexdigest() + extension\n",
        "    full_save_path = os.path.join(base_save_path, filename)\n",
        "\n",
        "    print(f\"Saving image to: {full_save_path}\")\n",
        "    image.save(full_save_path)\n",
        "    print(full_save_path)\n",
        "    #extract_all_features(full_save_path)\n",
        "    print(f\"Successfully saved to {full_save_path}\")\n",
        "  else:\n",
        "    print(f\"Failed to download image from {image_url}. Status code: {response.status_code}\")\n",
        "\n",
        "def extract_noise_features(image_path):\n",
        "    \"\"\"\n",
        "    Extract noise and frequency features from an image\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    # 1. NOISE RESIDUAL EXTRACTION\n",
        "    # Denoise the image\n",
        "    denoised = denoise_tv_chambolle(gray, weight=0.1)\n",
        "    # Extract noise by subtracting denoised from original\n",
        "    noise_residual = gray - denoised\n",
        "\n",
        "    # Noise statistics\n",
        "    features['noise_mean'] = np.mean(noise_residual)\n",
        "    features['noise_std'] = np.std(noise_residual)\n",
        "    features['noise_variance'] = np.var(noise_residual)\n",
        "\n",
        "    # 2. FREQUENCY DOMAIN ANALYSIS\n",
        "    # Apply FFT\n",
        "    fft = fft2(gray)\n",
        "    fft_shift = fftshift(fft)\n",
        "    magnitude_spectrum = np.abs(fft_shift)\n",
        "\n",
        "    # Divide into frequency bands\n",
        "    h, w = magnitude_spectrum.shape\n",
        "    center_h, center_w = h // 2, w // 2\n",
        "\n",
        "    # Low frequency (center 25%)\n",
        "    low_freq = magnitude_spectrum[\n",
        "        center_h - h//8:center_h + h//8,\n",
        "        center_w - w//8:center_w + w//8\n",
        "    ]\n",
        "    features['low_freq_energy'] = np.mean(low_freq)\n",
        "\n",
        "    # High frequency (outer edges)\n",
        "    high_freq_mask = np.ones_like(magnitude_spectrum)\n",
        "    high_freq_mask[\n",
        "        center_h - h//4:center_h + h//4,\n",
        "        center_w - w//4:center_w + w//4\n",
        "    ] = 0\n",
        "    high_freq = magnitude_spectrum * high_freq_mask\n",
        "    features['high_freq_energy'] = np.mean(high_freq[high_freq > 0])\n",
        "\n",
        "    # Frequency ratio (AI images often have unnatural frequency distribution)\n",
        "    features['freq_ratio'] = features['high_freq_energy'] / (features['low_freq_energy'] + 1e-10)\n",
        "\n",
        "    # 3. TEXTURE ANALYSIS\n",
        "    # Compute gradients\n",
        "    grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "\n",
        "    features['gradient_mean'] = np.mean(gradient_magnitude)\n",
        "    features['gradient_std'] = np.std(gradient_magnitude)\n",
        "\n",
        "    # 4. LOCAL VARIANCE ANALYSIS\n",
        "    # AI images often have unnatural smoothness in certain regions\n",
        "    kernel_size = 5\n",
        "    local_means = cv2.blur(gray.astype(float), (kernel_size, kernel_size))\n",
        "    local_sq_means = cv2.blur((gray.astype(float))**2, (kernel_size, kernel_size))\n",
        "    local_variance = local_sq_means - local_means**2\n",
        "\n",
        "    features['local_var_mean'] = np.mean(local_variance)\n",
        "    features['local_var_std'] = np.std(local_variance)\n",
        "\n",
        "    # 5. EDGE COHERENCE\n",
        "    # AI images may have inconsistent edges\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    features['edge_density'] = np.sum(edges > 0) / edges.size\n",
        "\n",
        "    return features\n",
        "\n",
        "def extract_all_features(image_path):\n",
        "    \"\"\"\n",
        "    Wrapper to extract all features and return as array\n",
        "    \"\"\"\n",
        "    features = extract_noise_features(image_path)\n",
        "    # Return as ordered array for ML model\n",
        "    return np.array([\n",
        "        features['noise_mean'],\n",
        "        features['noise_std'],\n",
        "        features['noise_variance'],\n",
        "        features['low_freq_energy'],\n",
        "        features['high_freq_energy'],\n",
        "        features['freq_ratio'],\n",
        "        features['gradient_mean'],\n",
        "        features['gradient_std'],\n",
        "        features['local_var_mean'],\n",
        "        features['local_var_std'],\n",
        "        features['edge_density']\n",
        "    ])\n",
        "\n",
        "IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp')\n",
        "\n",
        "# List all files and directories in the GDRIVE_SAVE_PATH\n",
        "all_files = os.listdir(GDRIVE_SAVE_PATH)\n",
        "\n",
        "# Filter for image files and store their full paths\n",
        "saved_image_paths = []\n",
        "for filename in all_files:\n",
        "    if filename.lower().endswith(IMAGE_EXTENSIONS):\n",
        "        full_path = os.path.join(GDRIVE_SAVE_PATH, filename)\n",
        "        saved_image_paths.append(full_path)\n",
        "\n",
        "\n",
        "urls = fetch_image_urls(\"coding on laptop\")\n",
        "if urls:\n",
        "  print(f\"Found {len(urls)} image URLs.\")\n",
        "  #for url in urls:\n",
        "   # download_and_save_images(url, GDRIVE_SAVE_PATH)\n",
        "print(f\"Found {len(saved_image_paths)} image files in {GDRIVE_SAVE_PATH}:\")\n",
        "for img_path in saved_image_paths:\n",
        "    extract_all_features(img_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-0740_6xIee",
        "outputId": "3cf21e25-fcba-48de-e5fb-a46ad12f48ae"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Images will be saved to: /content/drive/MyDrive/Unsplash_Images\n",
            "a\n",
            "hi\n",
            "here\n",
            "Found 10 image URLs.\n",
            "Found 10 image files in /content/drive/MyDrive/Unsplash_Images:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe1c2a82"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9efec402"
      },
      "source": [
        "First, we need to mount your Google Drive to be able to save files there. You will be prompted to authorize this connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61800e51",
        "outputId": "d9de87b5-0c50-4fe3-dcdf-49bf3e0991b7"
      },
      "source": [],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Images will be saved to: /content/drive/MyDrive/Unsplash_Images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19c3d343"
      },
      "source": [
        "# Task\n",
        "Extract features from all images saved in \"/content/drive/MyDrive/Unsplash_Images\" using the `extract_all_features` function and store these features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "683cc086"
      },
      "source": [
        "## List Saved Images\n",
        "\n",
        "### Subtask:\n",
        "List all image files within the `GDRIVE_SAVE_PATH` directory in your Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5b0b51b"
      },
      "source": [
        "**Reasoning**:\n",
        "To list all image files within the `GDRIVE_SAVE_PATH` directory, I will use `os.listdir()` to get all entries and then filter them based on common image file extensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00de89ec"
      },
      "source": [
        "# Task\n",
        "Conditionally mount Google Drive, then extract and store features for all images located in \"/content/drive/MyDrive/Unsplash_Images\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eebafc8b"
      },
      "source": [
        "## Conditionally Mount Google Drive\n",
        "\n",
        "### Subtask:\n",
        "Modify the code to check if Google Drive is already mounted before calling drive.mount(), avoiding redundant mounting attempts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1afd2dd"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to conditionally mount Google Drive. I will modify the existing code block to include a check using `os.path.ismount()` before calling `drive.mount()`, ensuring it only mounts if not already mounted. All other instructions regarding `GDRIVE_SAVE_PATH` and `os.makedirs` are already present in the original cell `C-0740_6xIee`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08930b07",
        "outputId": "8aac4e87-4f85-481a-8873-8cdafd1fe724"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import hashlib # Added for generating unique filenames\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.fftpack import fft2, fftshift\n",
        "from skimage.restoration import denoise_tv_chambolle\n",
        "\n",
        "from google.colab import userdata\n",
        "unsplashkey = userdata.get('Unsplash_Key')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Check if Google Drive is already mounted, if not, mount it\n",
        "if not os.path.ismount('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define the path where you want to save your images in Google Drive\n",
        "GDRIVE_SAVE_PATH = '/content/drive/MyDrive/Unsplash_Images'\n",
        "print(f\"Images will be saved to: {GDRIVE_SAVE_PATH}\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(GDRIVE_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "print(\"a\")\n",
        "\n",
        "def fetch_image_urls(query):\n",
        "\n",
        "  api_url =  f\"https://api.unsplash.com/search/photos/?query={query}&client_id={unsplashkey}\"\n",
        "  responses = requests.get(api_url)\n",
        "\n",
        "  if responses.status_code != 200:\n",
        "    print(\"Error fetching image data\")\n",
        "    return\n",
        "\n",
        "  data = responses.json()\n",
        "  print(\"hi\")\n",
        "  urls = [result[\"urls\"][\"regular\"] for result in data[\"results\"]]\n",
        "  print(\"here\")\n",
        "  return urls\n",
        "\n",
        "def download_and_save_images(image_url, base_save_path): # Renamed parameter for clarity\n",
        "  print(f\"Attempting to download: {image_url}\") # Clarified print statement\n",
        "  response = requests.get(image_url)\n",
        "  if response.status_code == 200:\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    #Determine file extension from content type or default to .jpg\n",
        "    content_type = response.headers.get('content-type', 'image/jpeg')\n",
        "    extension = '.' + content_type.split('/')[-1]\n",
        "\n",
        "    # Handle cases where content type might not provide a standard extension, or add more common types\n",
        "    if extension not in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:\n",
        "      extension = '.jpg' # Fallback extension\n",
        "\n",
        "    # Generate a unique filename using a hash of the URL\n",
        "    filename = hashlib.md5(image_url.encode('utf-8')).hexdigest() + extension\n",
        "    full_save_path = os.path.join(base_save_path, filename)\n",
        "\n",
        "    print(f\"Saving image to: {full_save_path}\")\n",
        "    image.save(full_save_path)\n",
        "    print(full_save_path)\n",
        "    #extract_all_features(full_save_path)\n",
        "    print(f\"Successfully saved to {full_save_path}\")\n",
        "  else:\n",
        "    print(f\"Failed to download image from {image_url}. Status code: {response.status_code}\")\n",
        "\n",
        "def extract_noise_features(image_path):\n",
        "    \"\"\"\n",
        "    Extract noise and frequency features from an image\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    # 1. NOISE RESIDUAL EXTRACTION\n",
        "    # Denoise the image\n",
        "    denoised = denoise_tv_chambolle(gray, weight=0.1)\n",
        "    # Extract noise by subtracting denoised from original\n",
        "    noise_residual = gray - denoised\n",
        "\n",
        "    # Noise statistics\n",
        "    features['noise_mean'] = np.mean(noise_residual)\n",
        "    features['noise_std'] = np.std(noise_residual)\n",
        "    features['noise_variance'] = np.var(noise_residual)\n",
        "\n",
        "    # 2. FREQUENCY DOMAIN ANALYSIS\n",
        "    # Apply FFT\n",
        "    fft = fft2(gray)\n",
        "    fft_shift = fftshift(fft)\n",
        "    magnitude_spectrum = np.abs(fft_shift)\n",
        "\n",
        "    # Divide into frequency bands\n",
        "    h, w = magnitude_spectrum.shape\n",
        "    center_h, center_w = h // 2, w // 2\n",
        "\n",
        "    # Low frequency (center 25%)\n",
        "    low_freq = magnitude_spectrum[\n",
        "        center_h - h//8:center_h + h//8,\n",
        "        center_w - w//8:center_w + w//8\n",
        "    ]\n",
        "    features['low_freq_energy'] = np.mean(low_freq)\n",
        "\n",
        "    # High frequency (outer edges)\n",
        "    high_freq_mask = np.ones_like(magnitude_spectrum)\n",
        "    high_freq_mask[\n",
        "        center_h - h//4:center_h + h//4,\n",
        "        center_w - w//4:center_w + w//4\n",
        "    ] = 0\n",
        "    high_freq = magnitude_spectrum * high_freq_mask\n",
        "    features['high_freq_energy'] = np.mean(high_freq[high_freq > 0])\n",
        "\n",
        "    # Frequency ratio (AI images often have unnatural frequency distribution)\n",
        "    features['freq_ratio'] = features['high_freq_energy'] / (features['low_freq_energy'] + 1e-10)\n",
        "\n",
        "    # 3. TEXTURE ANALYSIS\n",
        "    # Compute gradients\n",
        "    grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "\n",
        "    features['gradient_mean'] = np.mean(gradient_magnitude)\n",
        "    features['gradient_std'] = np.std(gradient_magnitude)\n",
        "\n",
        "    # 4. LOCAL VARIANCE ANALYSIS\n",
        "    # AI images often have unnatural smoothness in certain regions\n",
        "    kernel_size = 5\n",
        "    local_means = cv2.blur(gray.astype(float), (kernel_size, kernel_size))\n",
        "    local_sq_means = cv2.blur((gray.astype(float))**2, (kernel_size, kernel_size))\n",
        "    local_variance = local_sq_means - local_means**2\n",
        "\n",
        "    features['local_var_mean'] = np.mean(local_variance)\n",
        "    features['local_var_std'] = np.std(local_variance)\n",
        "\n",
        "    # 5. EDGE COHERENCE\n",
        "    # AI images may have inconsistent edges\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    features['edge_density'] = np.sum(edges > 0) / edges.size\n",
        "\n",
        "    return features\n",
        "\n",
        "def extract_all_features(image_path):\n",
        "    \"\"\"\n",
        "    Wrapper to extract all features and return as array\n",
        "    \"\"\"\n",
        "    features = extract_noise_features(image_path)\n",
        "    # Return as ordered array for ML model\n",
        "    return np.array([\n",
        "        features['noise_mean'],\n",
        "        features['noise_std'],\n",
        "        features['noise_variance'],\n",
        "        features['low_freq_energy'],\n",
        "        features['high_freq_energy'],\n",
        "        features['freq_ratio'],\n",
        "        features['gradient_mean'],\n",
        "        features['gradient_std'],\n",
        "        features['local_var_mean'],\n",
        "        features['local_var_std'],\n",
        "        features['edge_density']\n",
        "    ])\n",
        "\n",
        "IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp')\n",
        "\n",
        "# List all files and directories in the GDRIVE_SAVE_PATH\n",
        "all_files = os.listdir(GDRIVE_SAVE_PATH)\n",
        "\n",
        "# Filter for image files and store their full paths\n",
        "saved_image_paths = []\n",
        "for filename in all_files:\n",
        "    if filename.lower().endswith(IMAGE_EXTENSIONS):\n",
        "        full_path = os.path.join(GDRIVE_SAVE_PATH, filename)\n",
        "        saved_image_paths.append(full_path)\n",
        "\n",
        "\n",
        "#urls = fetch_image_urls(\"coding on laptop\")\n",
        "#if urls:\n",
        "  #print(f\"Found {len(urls)} image URLs.\")\n",
        "  #for url in urls:\n",
        "   # download_and_save_images(url, GDRIVE_SAVE_PATH)\n",
        "print(f\"Found {len(saved_image_paths)} image files in {GDRIVE_SAVE_PATH}:\")\n",
        "for img_path in saved_image_paths:\n",
        "    extract_all_features(img_path)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images will be saved to: /content/drive/MyDrive/Unsplash_Images\n",
            "a\n",
            "Found 10 image files in /content/drive/MyDrive/Unsplash_Images:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32531332"
      },
      "source": [
        "## Extract Features from Images\n",
        "\n",
        "### Subtask:\n",
        "Iterate through each identified image file path and call the `extract_all_features` function to compute and collect its features. Store these features, potentially along with the image path or filename, for future analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513dc7a7"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract features and store them in a structured format, I will initialize lists for features and filenames, loop through the image paths, extract features using the provided function, and then create a Pandas DataFrame with the collected data. This will involve importing pandas, which is not yet imported.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b601a59f",
        "outputId": "817b0651-36d9-480a-915d-5efb610ebb4b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "all_image_features = []\n",
        "image_filenames = []\n",
        "\n",
        "# Loop through each image_path in the saved_image_paths list\n",
        "for img_path in saved_image_paths:\n",
        "    # Extract features for the current image\n",
        "    features = extract_all_features(img_path)\n",
        "    all_image_features.append(features)\n",
        "\n",
        "    # Extract and append the base filename\n",
        "    filename = os.path.basename(img_path)\n",
        "    image_filenames.append(filename)\n",
        "\n",
        "# Define column names for the features (matching the order in extract_all_features)\n",
        "feature_column_names = [\n",
        "    'noise_mean', 'noise_std', 'noise_variance', 'low_freq_energy',\n",
        "    'high_freq_energy', 'freq_ratio', 'gradient_mean', 'gradient_std',\n",
        "    'local_var_mean', 'local_var_std', 'edge_density'\n",
        "]\n",
        "\n",
        "# Create a Pandas DataFrame from all_image_features\n",
        "features_df = pd.DataFrame(all_image_features, columns=feature_column_names)\n",
        "\n",
        "# Add the image_filenames as a new column\n",
        "features_df['filename'] = image_filenames\n",
        "\n",
        "# Print the head of the features_df DataFrame\n",
        "print(\"Extracted Features DataFrame:\")\n",
        "print(features_df.head())\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the path for the Parquet file in Google Drive\n",
        "PARQUET_SAVE_PATH = os.path.join(GDRIVE_SAVE_PATH, 'extracted_image_features.parquet')\n",
        "\n",
        "# Save the DataFrame to a Parquet file\n",
        "features_df.to_parquet(PARQUET_SAVE_PATH)\n",
        "\n",
        "print(f\"Features DataFrame saved to: {PARQUET_SAVE_PATH}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Features DataFrame:\n",
            "   noise_mean  noise_std  noise_variance  low_freq_energy  high_freq_energy  \\\n",
            "0   86.221932  68.309252     4666.153961     61437.946965       3012.875338   \n",
            "1  140.453426  57.885135     3350.688897     39386.807092       1560.980208   \n",
            "2   49.715317  39.244317     1540.116395     34425.526590       3179.313286   \n",
            "3   43.453145  40.779443     1662.962945     36388.046464       3730.131295   \n",
            "4  162.917528  98.862653     9773.824125    122334.831125       7941.542156   \n",
            "\n",
            "   freq_ratio  gradient_mean  gradient_std  local_var_mean  local_var_std  \\\n",
            "0    0.049039      35.703154     67.986288      147.848052     462.997649   \n",
            "1    0.039632      21.765040     59.185266       97.573995     453.295801   \n",
            "2    0.092353      24.870499     68.402541      125.267923     488.661658   \n",
            "3    0.102510      23.744966     69.456416      129.890470     520.288097   \n",
            "4    0.064916      48.097781    123.579924      457.165756    1553.897599   \n",
            "\n",
            "   edge_density                               filename  \n",
            "0      0.037486  6f23b50b9a91d8e3cc3b273eb3d7be31.jpeg  \n",
            "1      0.024559  7d30edb29dbfbc8dec8475b87338a5c2.jpeg  \n",
            "2      0.024437  48f598dc3ee059a7e5269397835d8b5d.jpeg  \n",
            "3      0.024267  9663ae5deed6399fde76444ed7066d7c.jpeg  \n",
            "4      0.051582  d7781c47f94877b29f0b5c1d1ce682a1.jpeg  \n",
            "Features DataFrame saved to: /content/drive/MyDrive/Unsplash_Images/extracted_image_features.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7be738c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that features have been extracted from all saved images and indicate where they are stored or how they can be accessed for further analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2339d69e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Yes, features have been successfully extracted from all saved images. These extracted features are stored in a Pandas DataFrame named `features_df`. This DataFrame can be accessed directly for further analysis.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Google Drive was successfully and conditionally mounted to `/content/drive`. The designated image save path was set to `/content/drive/MyDrive/Unsplash_Images`, and the creation of this directory was confirmed.\n",
        "*   The `extract_all_features` function processes images to compute 11 distinct features, including noise residuals (mean, standard deviation, variance), frequency domain analysis (low frequency energy, high frequency energy, frequency ratio), texture analysis (gradient mean, standard deviation), local variance (mean, standard deviation), and edge density.\n",
        "*   Features were extracted from all identified image files located in the `GDRIVE_SAVE_PATH`.\n",
        "*   All extracted features, along with their corresponding filenames, are consolidated into a Pandas DataFrame named `features_df`. The head of this DataFrame was printed, confirming its successful creation and population with the calculated features.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The `features_df` DataFrame is now ready for use in subsequent machine learning tasks, such as training a model for image classification or clustering, or for further statistical exploration of image characteristics.\n",
        "*   To ensure persistence and avoid reprocessing, the `features_df` DataFrame should be saved to a persistent storage location (e.g., CSV, Parquet, or Pickle file) within Google Drive.\n"
      ]
    }
  ]
}